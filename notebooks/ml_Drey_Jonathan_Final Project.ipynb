{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import gensim\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Brining in arxiv metadata as CSV\n",
    "#df = pd.read_csv(\"/Users/aster/Desktop/Fall 2020/ML/final_project/data/arxiv_metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up Paths for rest of Notebook\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "#'uncleaned' subset of from arxiv metadata snapshot. ML(cs.lg) and or AI(cs.ai) category tagged papers with \"fair\" in the title\n",
    "#/Users/aster/Desktop/fall_2020/ML/final_project/pratt_ml_final_project/data/arxiv_corpus_2020_11_23.csv\n",
    "snapshot = os.path.join(\"/Users/aster/Desktop/fall_2020/ML/final_project/data/arxiv_corpus_2020_11_23.csv\")\n",
    "\n",
    "#data after cleaning, lemmatization, bigrams, tokenization\n",
    "dataset = os.path.join(\"/Users/aster/Desktop/fall_2020/ML/final_project/data/arxiv_corpus_2020_12_8.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Subset Selection\n",
    "\n",
    "Now that we have the arxiv metadata snapshot as a csv we can make 2 corpus, one with abstract data and one with title; which we'll preprocess and fit our model. For this project we're particularly interested in how machine learning practitioners are thinking and talking about \"fairness\". We're trying to see if we can identify types of approaches to fairness in the form of subjects. \n",
    "\n",
    "Our data subset will include papers: \n",
    "- With \"fair\" in the title\n",
    "- With machine learning (cs.LG) and artificial intelligence (cs.AI) in the categories column\n",
    "\n",
    "In this part of the project required loading the arxiv metadata snaphot which was rather large. I've commented it out here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Papers that have fair in title\n",
    "#df=df[df['title'].str.lower().str.contains(\"fair\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Papers including fair in the title that are in our categories of interest\n",
    "df = df[df[\"categories\"].str.lower().str.contains(\"cs.ai\", \"cs.lg\")]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing subset to a csv file in the data folder\n",
    "df = pd.read_csv(snapshot, encoding = 'utf8')\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using a lambda function to apply str(ing).lower(case) to the abstract and title column. \n",
    "#Also making new columns for the \"cleaned\" abstract and title data. This way we hold unto our original data.\n",
    "#I'm using a lambda function because I need to apply this to multiple columns\n",
    "df[[\"abstract_clean\", \"title_clean\"]] = df[[\"abstract\", \"title\"]].apply(lambda x: x.str.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our subset data in one place we can move unto cleaning. \n",
    "\n",
    "- [x] I'd like to get rid of puntuations.  \n",
    "- [x] I'd like to get rid of \"/n\"\n",
    "- [x] I'd like to get rid of numbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "#creating a variable to hold all the string patterns we want to operate on with regex\n",
    "# \"|\" pipe is an \"or\" joing the pattern for:\n",
    "#punctuation \"[^\\w\\s]\"\n",
    "#new line marker \"\\n\"\n",
    "#and numerals \"\\d+\"\n",
    "#then apply .str.replace via lambda function\n",
    "pattern = '|'.join([\"[^\\w\\s]\", \"\\n\", \"\\d+\", \"[‘’“”…]\"])\n",
    "\n",
    "df[['title_clean','abstract_clean']] = df[['title_clean','abstract_clean']].apply(lambda x: x.str.replace(pattern, ' '))\n",
    "#df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Leading and Trailing White Space\n",
    "df[['title_clean','abstract_clean']] = df[['title_clean','abstract_clean']].apply(lambda x: x.str.strip())\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Cleaning - More Normalization and Tokenization\n",
    "\n",
    "Now that we have our corpus clean we can had into preprocessing for our model. \n",
    "Our Preprocessing Tasks are: \n",
    "\n",
    "- [x] Stop words\n",
    "- [x] Stemming/Lemmatization of words\n",
    "- [x] Tokenizing words - Creating Document Term Matrix (otherwise known as bag of words)\n",
    "- [] Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Stemming/Lemmatization \n",
    "#nltk.download('wordnet')\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models.phrases import Phraser\n",
    "\n",
    "\n",
    "t = nltk.tokenize.WhitespaceTokenizer()\n",
    "lem = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "#this function will tokenize AND THEN lemmatize values in a column\n",
    "def lem_text(text):\n",
    "    return [lem.lemmatize(w, 'v') for w in t.tokenize(text)]\n",
    "\n",
    "#df = pd.DataFrame([‘this was cheesy blessing’, 'she likes these books ', ‘wow this is great amazing’], columns=[‘text’])\n",
    "#print(df)\n",
    "\n",
    "df[\"title_clean\"] = df[\"title_clean\"].apply(lem_text)\n",
    "df[\"abstract_clean\"] = df[\"abstract_clean\"].apply(lem_text)\n",
    "\n",
    "#Now we have tokenized and lemmed columns of abstract adn title data, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting rid of Stop Words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop=stopwords.words(\"english\")\n",
    "\n",
    "df[\"abstract_clean\"]= df['abstract_clean'].apply(lambda x: [item for item in x if item not in stop])\n",
    "df['title_clean'] = df['title_clean'].apply(lambda x: [item for item in x if item not in stop])\n",
    "# Strangely I can't group these funtion together or it doesn't seem to work on both columns i'm tryin to select. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-grams: Bigrams, Trigrams, Probability\n",
    "\n",
    "In computational lingusitics/natural language processing an \"n-gram\" describes a sequence of n-items in a collection of text. \n",
    "\n",
    "So in the sentence:\n",
    "\n",
    "\"Machine Learning should reckon with fairness, justice, and equity.\"\n",
    "\n",
    "\"Machine\" - 1-gram (unigram)\n",
    "\n",
    "\"Machine, Learning\" - 2-gram (bigram)\n",
    "\n",
    "\"Machine, Learning, should\" - 3-gram (trigram)\n",
    "\n",
    "Machine, Learning, should, reckon\" - 4-(quadgram)\n",
    "\n",
    "etc. \n",
    "\n",
    "Currently the text we're looking at is tokenized as unigrams. So now we're going to try using bigrams and trigrams and see what sort of new results we can get. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###Setting Bigrams\n",
    "import gensim\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "\n",
    "def bigrams(text):\n",
    "    bigram = Phrases(text, min_count=1)\n",
    "    bigram_mod = Phraser(bigram)\n",
    "    return [bigram_mod[[x]] for x in text]\n",
    "\n",
    "df['title_clean']=df['title_clean'].apply(lambda x: (x))\n",
    "df['title_clean']=df[['title_clean']].apply(bigrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Setting bigrams for the abstract data. We'll primarily be looking at abstracts for our topics.\n",
    "df['abstract_clean']=df['abstract_clean'].apply(lambda x: (x))\n",
    "df['abstract_clean']=df[['abstract_clean']].apply(bigrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Was just checking to see if my lambda function was applied. It was but it's returning a generator object\n",
    "#I'll have to transform my text data back into a list. \n",
    "#df['abstract_clean'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(df['abstract_clean'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"title_clean\"] = df[\"title_clean\"].apply(lambda x: list(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"abstract_clean\"] = df[\"abstract_clean\"].apply(lambda x: list(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the 2nd paper's title to make sure it's a list of unigram/bigrams and not a generator object\n",
    "#df[\"title_clean\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Rewriting df variable with our cleaned dataset. \n",
    "df= pd.read_csv(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Word Cloud Generation Just to see:\n",
    "\n",
    "# Import the wordcloud library\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Join the different processed titles together.\n",
    "#all_title = ','.join(list(df[\"title_clean\"].values))\n",
    "\n",
    "# Create a Title WordCloud object\n",
    "cloud=WordCloud(background_color=\"black\", max_words=1000)\n",
    "                       \n",
    "# Generate a word cloud\n",
    "cloud.generate(df[\"title_clean\"].to_string())\n",
    "\n",
    "# Visualize the word cloud\n",
    "cloud.to_image()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Abstract WordCloud object\n",
    "cloud=WordCloud(background_color=\"black\", max_words=1000)\n",
    "                       \n",
    "# Generate a word cloud\n",
    "cloud.generate(df[\"abstract_clean\"].to_string())\n",
    "\n",
    "# Visualize the word cloud\n",
    "cloud.to_image()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple of intersting things we can begin to think about and question with this simple visualization. \n",
    "\n",
    "- We see words we expect to see like \"fairness\" and \"fair\"\n",
    "    - interstingly \"fair\" and \"fairness\" weren't lemmed to \"fair\" but we might consider them representing different uses/understandings \n",
    "        - where and how are we using \"fairness\" as opposed to \"fair\"?\n",
    "    - Also \"unfairness\" is interestingly small when compared to fair and fairness. SO those seem to be used at the same amounts (possibly even in similar functions/interchangeably?) but \"unfairness\"is beng used differently somewhat. \n",
    "    \n",
    "- The largest used terms are apparently functional terms liek \"model\" \"system\" \"classification\" etc. \n",
    "- It's interesting that words like \"social\" and \"representation\" have low occurence. \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling - Latent Dirlecht Allocation (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic Modeling: A statistical modeling of the abstract topics that occur within a document (i.e. topics are \"latent\" within the document). In topic modeling we can understand a document as not only a collection of text but a distribution of \"topic\". \n",
    "\n",
    "Latent Dirlecht Allocation (LDA) is one type of model used in Topic Modelling. Our LDA model will build \"topics\" identify as cluster of words/tokens and then look at the distribution of topics throughout our documents. \n",
    "\n",
    "Our tasks in this section will be:\n",
    "- Preprocessing \n",
    "    - We'll preparing our data by creating a **Bag of Words** representation of our cleaned text data\n",
    "        \n",
    "            - the bag of words will require us to:\n",
    "                1. Make dictionary with all of the words in our corpus, numbered (and perhaps in alphabetical order)\n",
    "                2. A count of how often each word appears in each document\n",
    "                \n",
    "- Model Fitting:\n",
    "    - We'll build a pipeline \n",
    "Here we'll build a pipeline for fitting the LDA model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returning to dictionary buildig let's look at our abstract data. Our LDA will require 3 things:\n",
    "\n",
    "1. a dictionary - this will hold all the possible words (and in this second trial bi-grams) in our text with an id. \n",
    "\n",
    "2. a corpus - this is our colecltion of text, in this case the abstract_clean column\n",
    "\n",
    "3. a Count of our term-document frequency. So the count of how many times a term (word or bigram) appears in a document (row of data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Making a Document Term Matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv= CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "title_cv = cv.fit_transform(df['title_clean'])\n",
    "\n",
    "dtm = pd.DataFrame(title_cv.toarray(), columns=cv.get_feature_names())\n",
    "dtm.index = df.index\n",
    "\n",
    "#Here is our document-term matrix \n",
    "dtm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apparently Gensim requires a term document matrix, where the index is our terms and the columns are documents.\n",
    "tdm=dtm_df.transpose()\n",
    "tdm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"title_clean\"][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working with Title Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "   result = []\n",
    "   for token in gensim.utils.simple_preprocess(text):\n",
    "      #if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 2:\n",
    "       if len(token) > 1: \n",
    "         result.append(token)\n",
    "   return result\n",
    "\n",
    "processed = df[\"title_clean\"].map(preprocess)\n",
    "\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import matutils, models\n",
    "import scipy.sparse\n",
    "\n",
    "#transposing our document term matrix to a form gensim accepts\n",
    "#so wher before x axis was a document and y axis was a word, these have been switched\n",
    "tdm=dtm.transpose()\n",
    "\n",
    "sparse_counts = scipy.sparse.csr_matrix(dtm)\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#texts = [[word for word in df[\"title_clean\"].split()]] \n",
    "#for document in documents]\n",
    "#dictionary = corpora.Dictionary(texts)\n",
    "#corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "\n",
    "\n",
    "#title_dict = gensim.corpora.Dictionary(df[\"title_clean\"])\n",
    "\n",
    "#creating a bag of words of just title information\n",
    "#title_corpus = [title_dict.doc2bow(x) for x in df[\"title_clean\"]]\n",
    "#title_corpus = [title_dict.doc2bow(x) for x in texts]\n",
    "\n",
    "#Instantiating LDA model\n",
    "lda_t = gensim.models.ldamodel.LdaModel(id2word=dictionary, corpus=corpus, num_topics=5, passes=10)\n",
    "\n",
    "#Putting our Title topics in a dataframe\n",
    "title_topics = pd.DataFrame(lda_t.print_topics(num_words=10))\n",
    "title_topics.columns= [\"topics\", \"terms\"]\n",
    "title_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"title_clean\"][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working with Abstract Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now doing everything we just did but for abstract data:\n",
    "\n",
    "#First getting a document term matrix and setting a sparse matrix corpus\n",
    "#cv = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "ab_cv = cv.fit_transform(df['abstract_clean'])\n",
    "\n",
    "ab_dtm = pd.DataFrame(ab_cv.toarray(), columns=cv.get_feature_names())\n",
    "ab_dtm.index = df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next creating a sparse matrix corpus\n",
    "s_counts = scipy.sparse.csr_matrix(ab_dtm)\n",
    "ab_corpus = matutils.Sparse2Corpus(s_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating our dictionary of tokens with ids\n",
    "def preprocess(text):\n",
    "   result = []\n",
    "   for token in gensim.utils.simple_preprocess(text):\n",
    "      \n",
    "       if len(token) > 1: \n",
    "         result.append(token)\n",
    "   return result\n",
    "\n",
    "abstracts = df[\"abstract_clean\"].map(preprocess)\n",
    "\n",
    "ab_dictionary = gensim.corpora.Dictionary(processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Finally instantiating LDA model and fitting to our abstract data. \n",
    "lda = gensim.models.ldamodel.LdaModel(id2word=ab_dictionary, corpus=ab_corpus, num_topics=3, passes=30)\n",
    "\n",
    "#Putting our Title topics in a dataframe\n",
    "abstract_topics = pd.DataFrame(lda.print_topics(num_words=10))\n",
    "abstract_topics.columns= [\"topics\", \"terms\"]\n",
    "abstract_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving everything\n",
    "#abstract_topics.to_csv(\"/Users/aster/Desktop/fall_2020/ML/final_project/pratt_ml_final_project/data/abstract_topics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
